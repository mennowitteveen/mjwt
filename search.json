[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mjwt",
    "section": "",
    "text": "Hi there you have reached my illusterous python package containing my own tools"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "mjwt",
    "section": "Install",
    "text": "Install\npip install mjwt"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "mjwt",
    "section": "How to use",
    "text": "How to use\nIm now gonna do a short code example:\n\n\n\n2\n\n\n\nimport numpy as np\nfrom mjwt.utils import implot\nimplot(np.random.randn(5,5), show=True)\n\n[6.72, 4.8] 1 1\n\n\n\n\n\nBang, an implot above. There are other function inside of mjwt.utils and there are notebooks with snippets in ./nbs/ which are mirrored on mennowitteveen.github.io/mjwt/.\nHave a great day!"
  },
  {
    "objectID": "ipynb_snippets.html",
    "href": "ipynb_snippets.html",
    "title": "ipynb_snippets",
    "section": "",
    "text": "The notebook book contains yada yada yada. It is the philosophy to have all the snippets in the notebook work, with needed data being present in mjwt repo. In some cases this might not possible."
  },
  {
    "objectID": "ipynb_snippets.html#matplotlib-tricks",
    "href": "ipynb_snippets.html#matplotlib-tricks",
    "title": "ipynb_snippets",
    "section": "Matplotlib tricks",
    "text": "Matplotlib tricks\n\n# Use this to modifier your axis to your hearts contend:\nplt.ylim(-2, 2)\nplt.xlim(0,10)\n\nx1,x2,y1,y2 = plt.axis()\nplt.axis((x1,x2,25,250))\n\n(0.0, 10.0, 25.0, 250.0)\n\n\n\n\n\n\n## Hairy fat-tailed distribution?: Use a linspace to set the buckets of a histogram nicely:\nn = 10**5; \nstds = np.sqrt(1/np.random.rand(n)*400);\nws = pd.DataFrame(np.random.randn(n)*stds) # Rather weird fat-tailed weight distribution:\nws.hist(bins=np.linspace(-200,200,200), figsize=[10,3]); # &lt;-- without the bins argument this is a mess"
  },
  {
    "objectID": "ipynb_snippets.html#pandas",
    "href": "ipynb_snippets.html#pandas",
    "title": "ipynb_snippets",
    "section": "Pandas",
    "text": "Pandas\n\n# Load coef_df\ncoef_df = pd.read_hdf('coef_df.h5','coef_df')\n\n\ncoef_df.plot.scatter(x=0,y=1)\n\n&lt;Axes: xlabel='ga_totdays', ylabel='mat_age'&gt;\n\n\n\n\n\n\ncoef_df.describe()\n\n\n\n\n\n\n\n\nga_totdays\nmat_age\nmat_bmi\nmpop_white\nis_twin\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n-0.288608\n-0.134522\n-0.238682\n-0.336548\n0.317581\n\n\nstd\n0.061273\n0.053639\n0.052149\n0.069799\n0.064057\n\n\nmin\n-0.475273\n-0.305157\n-0.385548\n-0.557403\n0.012321\n\n\n25%\n-0.319592\n-0.168062\n-0.275030\n-0.381171\n0.284586\n\n\n50%\n-0.279579\n-0.140471\n-0.240565\n-0.343076\n0.322209\n\n\n75%\n-0.250055\n-0.102844\n-0.205305\n-0.298686\n0.355801\n\n\nmax\n-0.118547\n-0.000000\n-0.000000\n-0.000000\n0.544112\n\n\n\n\n\n\n\n\n# ?sns.pairplot\n\n\nplt.figure();\ncoef_df.plot.hist(alpha=0.8, bins=20); \nplt.show()\n\n&lt;Figure size 1000x500 with 0 Axes&gt;\n\n\n\n\n\n\n# For scattermatrix plot stuff\nsns.pairplot(coef_df, plot_kws={\"s\": 1.0})\n\n\n\n\n\n\n\nimage.png\n\n\n\ncoef_df\n\n\n\n\n\n\n\n\nga_totdays\nmat_age\nmat_bmi\nmpop_white\nis_twin\n\n\n\n\n0\n-0.392195\n-0.148588\n-0.314482\n-0.387349\n0.317503\n\n\n1\n-0.386727\n-0.189583\n-0.313807\n-0.400882\n0.378390\n\n\n2\n-0.290056\n-0.160036\n-0.225576\n-0.349331\n0.240991\n\n\n3\n-0.238261\n-0.084640\n-0.230761\n-0.321250\n0.225304\n\n\n4\n-0.252113\n-0.056818\n-0.176889\n-0.270960\n0.355325\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n-0.356478\n-0.129357\n-0.299575\n-0.343666\n0.278307\n\n\n996\n-0.272725\n-0.133474\n-0.177013\n-0.276120\n0.290608\n\n\n997\n-0.245847\n-0.077554\n-0.235886\n-0.437620\n0.246134\n\n\n998\n-0.281898\n-0.164914\n-0.212031\n-0.351553\n0.304425\n\n\n999\n-0.274264\n-0.115117\n-0.199899\n-0.205464\n0.253323\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n# coef_df.plot.scatter(x=0,y=1)\n\n\n# This is almost the way.\ncoef_df.apply(lambda x: sns.distplot(x, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 3, 'label' : x.name}));\n\n/tmp/ipykernel_1235/925458556.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  coef_df.apply(lambda x: sns.distplot(x, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 3, 'label' : x.name}));\n/opt/conda/lib/python3.11/site-packages/seaborn/distributions.py:2511: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  kdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\n/tmp/ipykernel_1235/925458556.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  coef_df.apply(lambda x: sns.distplot(x, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 3, 'label' : x.name}));\n/opt/conda/lib/python3.11/site-packages/seaborn/distributions.py:2511: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  kdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\n/tmp/ipykernel_1235/925458556.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  coef_df.apply(lambda x: sns.distplot(x, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 3, 'label' : x.name}));\n/opt/conda/lib/python3.11/site-packages/seaborn/distributions.py:2511: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  kdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\n/tmp/ipykernel_1235/925458556.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  coef_df.apply(lambda x: sns.distplot(x, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 3, 'label' : x.name}));\n/opt/conda/lib/python3.11/site-packages/seaborn/distributions.py:2511: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  kdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\n/tmp/ipykernel_1235/925458556.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  coef_df.apply(lambda x: sns.distplot(x, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 3, 'label' : x.name}));\n/opt/conda/lib/python3.11/site-packages/seaborn/distributions.py:2511: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  kdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\n\n\n\n\n\n\n# This is almost the way.\ncoef_df.apply(lambda x: sns.kdeplot(x, kde_kws = {'shade': True, 'linewidth': 3, 'label' : x.name}));\n\nAttributeError: Line2D.set() got an unexpected keyword argument 'kde_kws'\n\n\n\n\n\n\ncoef_df.apply(lambda x: sns.kdeplot(x, **{'fill': True, 'linewidth': 3, 'label' : x.name})); plt.legend()\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\n\n# If you want this without warnings, you need to go and update it (\"distplot\" is getting removed).\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    coef_df.apply(lambda x: sns.distplot(x, hist = True, kde = True, hist_kws={\"alpha\":0.3, \"linewidth\": 3}, \n                                         kde_kws = {'shade': False, 'linewidth': 3, 'label' : x.name, 'alpha':0.99}));\n    plt.legend()\n\n\n\n\n\n# Load Iris data:\niris = sns.load_dataset(\"iris\")\n\n# Vanilla\niris.plot.scatter(x='sepal_length',y='petal_width',c='petal_length',cmap='bwr', figsize=(1,1),ax=plt.subplot(111)); plt.show()\niris.plot.scatter(x='sepal_length',y='petal_width',c='petal_length',cmap='winter', figsize=(1,1),ax=plt.subplot(111)); plt.show()\n\n# plt.figure()\n#iris.plot(subplots=True, layout=(1,4)) ## Does not work as desired\n\n# This works! -&gt; Excellent!!\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,3))\niris.plot.scatter(x='sepal_length',y='petal_width',c='petal_length',cmap='seismic', ax=axes[0])\niris.plot.scatter(x='sepal_length',y='petal_width',c='petal_length',cmap='jet', ax=axes[1])\niris.plot.scatter(x='sepal_length',y='petal_width',c='petal_length',cmap='rainbow', ax=axes[2])\nplt.show()"
  },
  {
    "objectID": "ipynb_snippets.html#viz-config-stuff-for-pdsnsplt",
    "href": "ipynb_snippets.html#viz-config-stuff-for-pdsnsplt",
    "title": "ipynb_snippets",
    "section": "Viz config stuff for pd+sns+plt",
    "text": "Viz config stuff for pd+sns+plt\nI used common package abbreviation in this sub-title.\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\n# use this to display stuff:\npd.options.display.max_columns = 50\npd.options.display.precision"
  },
  {
    "objectID": "ipynb_snippets.html#manhattan-plots",
    "href": "ipynb_snippets.html#manhattan-plots",
    "title": "ipynb_snippets",
    "section": "Manhattan Plots",
    "text": "Manhattan Plots\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform, randint\ndf = []\n# Generate Manhattan plot: (#optional tweaks for relplot: linewidth=0, s=9)\ndef manhattan_plot(*, data_df, x, y, regcol='chrom'):\n    plot = sns.relplot(data=data_df, x=x, y=y, aspect=4, hue=regcol, palette = 'bright', legend=None) \n    chrom_df=data_df.groupby(regcol)[x].median()\n    plot.ax.set_xlabel(regcol); plot.ax.set_xticks(chrom_df); plot.ax.set_xticklabels(chrom_df.index)\n    plot.fig.suptitle('Manhattan plot')\n    plt.show()\n\n# Simulate DataFrame (in \"plink bim like\" format).\ngwas_df = pd.DataFrame({\n'rsid'  : ['rs{}'.format(i) for i in np.arange(10000)],\n'pval'  : uniform.rvs(size=10000),\n'chrom' : [i for i in randint.rvs(0,23,size=10000)],\n'pos'   : [i for i in randint.rvs(0,10**5,size=10000)]})\ngwas_df['-logp'] = -np.log10(gwas_df.pval)\ngwas_df = gwas_df.sort_values(['chrom','pos']); gwas_df.reset_index(inplace=True, drop=True); \ngwas_df['i'] = gwas_df.index\n\ndrift=4000\ndef fun(arg): # Function to create sampling gap in the middle of the chromosome.\n    if (arg &gt; (50000-drift)) and (arg &lt; (50000 + drift)):\n        arg = randint.rvs(0,10**5,size=1)[0]\n    return arg\ngwas_df['pos'] = gwas_df['pos'].apply(fun)\ngwas_df['pos'] = gwas_df['pos'].apply(fun)\n# arr = np.array([res_dt['neur.train']['C'].flatten(),res_dt['neur.train']['P'].flatten()]).T\n# print('---')\n# man_df = pd.DataFrame(arr, columns=['corr','pval'], index=bim_df.index.copy())\n# man_df = bim_df.merge(man_df, left_index=True, right_index=True)\n# man_df['-logp'] = -np.log10(man_df.pval)\n\n# Create a good cumpos!\nchrom_df = gwas_df.groupby('chrom')[['pos']].max().rename(columns=dict(pos='maxpos'))\nchrom_df = chrom_df.cumsum() - chrom_df #df.iloc[0,0]\ngwas_df = gwas_df.merge(chrom_df, left_on='chrom', right_on='chrom')\ngwas_df['cumpos'] = gwas_df['pos'] + gwas_df['maxpos']\ngwas_df['i']= gwas_df.index\n\n# Plot those Manhattans!:\nmanhattan_plot(data_df=gwas_df, x='i',      y='-logp')\nmanhattan_plot(data_df=gwas_df, x='cumpos', y='-logp')"
  },
  {
    "objectID": "ipynb_snippets.html#numpy",
    "href": "ipynb_snippets.html#numpy",
    "title": "ipynb_snippets",
    "section": "Numpy",
    "text": "Numpy\nShould populate this later."
  },
  {
    "objectID": "ipynb_snippets.html#pandas-1",
    "href": "ipynb_snippets.html#pandas-1",
    "title": "ipynb_snippets",
    "section": "Pandas",
    "text": "Pandas\n\ndf = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n   ...:                           'foo', 'bar', 'foo', 'foo'],\n   ...:                    'B' : ['one', 'one', 'two', 'three',\n   ...:                           'two', 'two', 'one', 'three'],\n   ...:                    'C' : np.random.randn(8),\n   ...:                    'D' : np.random.randn(8)})\n\ndef fun(arg):\n    return arg.copy()\ndf.groupby('A').apply(fun)\n# Sometimes ONE HAS TO USE AGG() AND PD.SERIES TO GET IT TO WORK\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\nA\n\n\n\n\n\n\n\n\n\nbar\n1\nbar\none\n1.593198\n-1.106698\n\n\n3\nbar\nthree\n-0.136551\n-1.285232\n\n\n5\nbar\ntwo\n-0.091798\n-0.639158\n\n\nfoo\n0\nfoo\none\n1.312274\n0.644841\n\n\n2\nfoo\ntwo\n-0.224747\n0.654681\n\n\n4\nfoo\ntwo\n0.123852\n1.207584\n\n\n6\nfoo\none\n1.142278\n-1.296145\n\n\n7\nfoo\nthree\n0.222377\n-0.993053\n\n\n\n\n\n\n\n\ndf.select_dtypes([np.number]).head()\n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n0\n1.312274\n0.644841\n\n\n1\n1.593198\n-1.106698\n\n\n2\n-0.224747\n0.654681\n\n\n3\n-0.136551\n-1.285232\n\n\n4\n0.123852\n1.207584\n\n\n\n\n\n\n\n\n# Define the data\nres_dt = {\n    'mkey1': {\n        'rkey1': [1, 2, 3],\n        'rkey2': [4, 5, 6]\n    },\n    'mkey2': {\n        'rkey3': [7, 8, 9],\n        'rkey4': [10, 11, 12]\n    }\n}\n\n# Define the 'fun' function\ndef fun(arg):\n    return arg\n\n# Apply the formula to create mod_df\nperfnum = None\nmod_df = pd.Series({(mkey, rkey, 'cv' + str(cvi)): fun(perfnum) \n                   for mkey, res1_dt in res_dt.items()\n                   for rkey, res2_arr in res1_dt.items()\n                   for cvi, perfnum in enumerate(res2_arr)})\nmod_df = pd.DataFrame(mod_df, columns=['base'])\nmod_df.index.set_names(['model', 'repit', 'fold'], inplace=True)\n\ndisplay(mod_df, mod_df['base'].groupby(['model']).agg([np.mean,np.std]).sort_values('mean'))\ndisplay(mod_df['base'].groupby(['model']).agg([np.mean,np.std]).sort_values('mean')) #.head())\ndisplay(mod_df['base'].unstack('model').apply([np.mean,np.std]))\n\n\n\n\n\n\n\n\n\n\nbase\n\n\nmodel\nrepit\nfold\n\n\n\n\n\nmkey1\nrkey1\ncv0\n1\n\n\ncv1\n2\n\n\ncv2\n3\n\n\nrkey2\ncv0\n4\n\n\ncv1\n5\n\n\ncv2\n6\n\n\nmkey2\nrkey3\ncv0\n7\n\n\ncv1\n8\n\n\ncv2\n9\n\n\nrkey4\ncv0\n10\n\n\ncv1\n11\n\n\ncv2\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nmodel\n\n\n\n\n\n\nmkey1\n3.5\n1.870829\n\n\nmkey2\n9.5\n1.870829\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nmodel\n\n\n\n\n\n\nmkey1\n3.5\n1.870829\n\n\nmkey2\n9.5\n1.870829\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nmkey1\nmkey2\n\n\n\n\nmean\n3.500000\n9.500000\n\n\nstd\n1.870829\n1.870829\n\n\n\n\n\n\n\n\n.xs('something', level='lvl')\n\n\n.filter(like='something', axis=1)\n\n\n# apply, transform, agg, filter\n\n\ndf.rename(index={\"one\": \"two\", \"y\": \"z\"})\nIn [107]: s.index.set_names(['L1', 'L2'], inplace=True)\n\n\n.unstack('model')\ntst_df.reorder_levels([2,1,0]).sort_index()\n\n\nmod_df['base'].groupby(['model']).agg([np.mean,np.std]).sort_values('mean').head()\nmod_df['base'].unstack('model').apply([np.mean,np.std])\n\n\npd.read\n\n\n# Nested performace estimation code:\n\n## This works, but it contains waaay 2 much thinkering.\n#nlst = ['hghgfg']\ndef w(fun): return fun #lambda x: fun(x)\ndef fun0(df): return pd.Series(df.apply(np.concatenate).values.mean(),index=['mean'])\ndef fun1(df):\n    #nlst[0] = df.copy()\n    v = df.apply(np.concatenate).values.std()\n    return pd.Series(v,index=['std'])\n\nfun_lst = [w(eval('fun'+str(i))) for i in range(2)]\n\ndef meta(df):\n    r_lst = []\n    for fun in fun_lst:\n        r_lst.append(fun(df))\n    return pd.concat(r_lst)\n\nperf_df.groupby('model').apply(meta).sort_values('mean').iloc[::-1].head(10)\n\n\ndef fun(arg):\n    return arg.copy()\nthis_df.set_index('n_pca').groupby('shift').apply(fun).unstack('shift')['r2'].plot()\n\nNameError: name 'this_df' is not defined\n\n\n\ndf = pd.DataFrame({ \"manager\": [\"Johns;Tim \", \"Mcgregor; Dave\", \"DeRocca; Leo\", \"Haze; Jim\"] ,\n                     \"target\": [42000, 85000, 45000, 33000]}); display(df)\ndf[['last_name','first_name']] =  df['manager'].str.split(\";\", n=1, expand=True) # n is max number of splits\ndf\n\n\n\n\n\n\n\n\nmanager\ntarget\n\n\n\n\n0\nJohns;Tim\n42000\n\n\n1\nMcgregor; Dave\n85000\n\n\n2\nDeRocca; Leo\n45000\n\n\n3\nHaze; Jim\n33000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmanager\ntarget\nlast_name\nfirst_name\n\n\n\n\n0\nJohns;Tim\n42000\nJohns\nTim\n\n\n1\nMcgregor; Dave\n85000\nMcgregor\nDave\n\n\n2\nDeRocca; Leo\n45000\nDeRocca\nLeo\n\n\n3\nHaze; Jim\n33000\nHaze\nJim\n\n\n\n\n\n\n\n\n# Explode dict colunms:\ndf = pd.DataFrame({\"A\":[{\"a\":3},{\"b\":4,\"c\":5}], \"B\":[6,7]})\ndisplay(df)\ndf['A'].apply(pd.Series); display(_)\ndf = pd.concat([df, df[\"A\"].apply(pd.Series)], axis=1); display(df)\n\nprint('-- Another way to explode dicts: --')\ndf = pd.DataFrame({\"A\":[{\"a\":3},{\"b\":4,\"c\":5}], \"B\":[6,7]})\npd.DataFrame(df['A'].to_list()) \n\nprint('-- 2 more ways --')\ndf.join(pd.json_normalize(df.A)); display(_)\ndf.join(df.A.apply(pd.Series)); display(_)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n{'a': 3}\n6\n\n\n1\n{'b': 4, 'c': 5}\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmanager\ntarget\nlast_name\nfirst_name\n\n\n\n\n0\nJohns;Tim\n42000\nJohns\nTim\n\n\n1\nMcgregor; Dave\n85000\nMcgregor\nDave\n\n\n2\nDeRocca; Leo\n45000\nDeRocca\nLeo\n\n\n3\nHaze; Jim\n33000\nHaze\nJim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\na\nb\nc\n\n\n\n\n0\n{'a': 3}\n6\n3.0\nNaN\nNaN\n\n\n1\n{'b': 4, 'c': 5}\n7\nNaN\n4.0\n5.0\n\n\n\n\n\n\n\n-- Another way to explode dicts: --\n-- 2 more ways --\n\n\n\n\n\n\n\n\n\nmanager\ntarget\nlast_name\nfirst_name\n\n\n\n\n0\nJohns;Tim\n42000\nJohns\nTim\n\n\n1\nMcgregor; Dave\n85000\nMcgregor\nDave\n\n\n2\nDeRocca; Leo\n45000\nDeRocca\nLeo\n\n\n3\nHaze; Jim\n33000\nHaze\nJim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmanager\ntarget\nlast_name\nfirst_name\n\n\n\n\n0\nJohns;Tim\n42000\nJohns\nTim\n\n\n1\nMcgregor; Dave\n85000\nMcgregor\nDave\n\n\n2\nDeRocca; Leo\n45000\nDeRocca\nLeo\n\n\n3\nHaze; Jim\n33000\nHaze\nJim\n\n\n\n\n\n\n\n\n# pd.DataFrame(df['A'].to_list())"
  },
  {
    "objectID": "ipynb_snippets.html#pytorch-101",
    "href": "ipynb_snippets.html#pytorch-101",
    "title": "ipynb_snippets",
    "section": "Pytorch 101",
    "text": "Pytorch 101\n\n#  -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# For this example, the output y is a linear function of (x, x^2, x^3), so\n# we can consider it as a linear layer neural network. Let's prepare the\n# tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n# of shape (2000, 3) \n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. The Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\n# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n# to match the shape of `y`.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-6\nfor t in range(2000):\n\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(xx)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n# You can access the first layer of `model` like accessing the first item of a list\nlinear_layer = model[0]\n\n# For linear layer, its parameters are stored as `weight` and `bias`.\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n\n99 505.64410400390625\n199 337.98028564453125\n299 226.92686462402344\n399 153.361083984375\n499 104.6224136352539\n599 72.3280029296875\n699 50.92662048339844\n799 36.74208450317383\n899 27.339014053344727\n999 21.10478401184082\n1099 16.970691680908203\n1199 14.228747367858887\n1299 12.409767150878906\n1399 11.202799797058105\n1499 10.401751518249512\n1599 9.87000560760498\n1699 9.516922950744629\n1799 9.282390594482422\n1899 9.126569747924805\n1999 9.023015975952148\nResult: y = 0.004640887025743723 + 0.8434610962867737 x + -0.0008006299030967057 x^2 + -0.09144145995378494 x^3"
  },
  {
    "objectID": "ipynb_snippets.html#keras-snippet",
    "href": "ipynb_snippets.html#keras-snippet",
    "title": "ipynb_snippets",
    "section": "Keras Snippet",
    "text": "Keras Snippet\n\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Model, Sequential\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, RANSACRegressor, TheilSenRegressor, HuberRegressor, LassoCV, LarsCV, Lars\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.metrics import auc, make_scorer, roc_auc_score, r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom scipy.stats import pearsonr, spearmanr\n# from keras import Sequential\n# from keras.layers import Dense, Dropout\nres_dt = dict()\ndef pearson_score(y_true, y_pred):\n    return pearsonr(y_true, y_pred)[0]\n\ndef spearman_score(y_true, y_pred):\n    return spearmanr(y_true, y_pred)[0]\n    \ndef create_model(optimizer='sgd',\n                 kernel_initializer='glorot_uniform', \n                 dropout=0.5):\n    model = Sequential()\n\n    model.add(Dense(10,activation='tanh',kernel_initializer=kernel_initializer))\n    model.add(Dropout(dropout))\n    model.add(Dense(10,activation='tanh',kernel_initializer=kernel_initializer))\n    model.add(Dropout(dropout))\n#     model.add(Dropout(dropout))\n#     model.add(Dense(128,activation='tanh',kernel_initializer=kernel_initializer))\n#     model.add(Dropout(dropout))\n    model.add(Dense(1,activation='linear',kernel_initializer=kernel_initializer))\n\n    model.compile(loss='mean_squared_error',optimizer=optimizer, metrics=['accuracy'])\n\n    return model\n\n# wrap the model using the function you created\ndnn = KerasRegressor(build_fn=create_model, verbose=0)\n\n# just create the pipeline\n# pipeline = Pipeline([\n#     ('clf',clf)\n# ])\n\n# pipeline.fit(X_train, y_train)"
  },
  {
    "objectID": "ipynb_snippets.html#gaussian-processes",
    "href": "ipynb_snippets.html#gaussian-processes",
    "title": "ipynb_snippets",
    "section": "Gaussian Processes",
    "text": "Gaussian Processes\n\n##################################################\n## Basic setup\nfrom sklearn.datasets import load_wine\nwine_dt = load_wine()\nwine_df = pd.DataFrame(wine_dt['data'],columns=wine_dt['feature_names'])\n\nX = wine_df.values\nn,p = X.shape\nSigma = X.T.dot(X)/n; print('Sigma.shape -&gt; ', Sigma.shape)\n\n\n###################################################\n## Generate Basic sample from Multivariate Normal:\nx = np.random.multivariate_normal(np.zeros(p),Sigma)\nprint(x)\n\nSigma.shape -&gt;  (13, 13)\n[3.89132606e+01 8.12103938e+00 6.87224463e+00 5.69947022e+01\n 3.05815577e+02 7.50441890e+00 6.04952658e+00 7.70976236e-01\n 4.46277373e+00 1.11720361e+01 2.68117246e+00 8.21249076e+00\n 1.90230019e+03]\n\n\n\n## Generate from Kernels: RBF\n####################################################\n\np = 300\nX = np.linspace(0.0,1.0,p)[:,np.newaxis]\nline_df = pd.DataFrame(X)\n\nl = 0.03\ns = 1.0\ndef fun(a,b):\n    return (s**2.0)*np.exp(-((a-b)**2)/(2*l**2))\n\nS = line_df.T.corr(method=fun)\nplt.figure(figsize=[8,4]); plt.imshow(S); plt.colorbar(); plt.show()\n\nx_smp = np.random.multivariate_normal(np.zeros(p),S) # Native cholesky from np.linalg has issues, this uses svd\nplt.figure(figsize=[8,4]); plt.plot(x_smp); plt.show()\n\n\n\n\n\n\n\n\n## Generate from Kernels: RBF\n####################################################\n\np = 300\nX = np.linspace(0.0,1.0,p)[:,np.newaxis]\nline_df = pd.DataFrame(X)\n\nper = .2\nl = 0.35\ns= 1.0\ndef fun(a,b):\n    return (s**2.0)*np.exp((-2*np.sin(np.pi*np.abs(a-b)/per)**2)/(2*l**2))\n\nS = line_df.T.corr(method=fun)\nplt.figure(figsize=[8,4]); plt.imshow(S); plt.colorbar(); plt.show()\n\nx_smp = np.random.multivariate_normal(np.zeros(p),S) # Native cholesky from np.linalg has issues, this uses svd\nplt.figure(figsize=[8,4]); plt.plot(x_smp); plt.show()"
  },
  {
    "objectID": "ipynb_snippets.html#recipe-resources",
    "href": "ipynb_snippets.html#recipe-resources",
    "title": "ipynb_snippets",
    "section": "Recipe Resources",
    "text": "Recipe Resources\n\nAwesome Brit @ pydata Amsterdam, debugging ML models: https://www.youtube.com/watch?v=DkLPYccEJ8Y\nmaybe mildly interesting: https://www.youtube.com/watch?v=kbj3llSbaVA similar topic"
  },
  {
    "objectID": "ipynb_snippets.html#ram-usage",
    "href": "ipynb_snippets.html#ram-usage",
    "title": "ipynb_snippets",
    "section": "RAM usage:",
    "text": "RAM usage:\n\nimport os, psutil\nprocess = psutil.Process(os.getpid())\nprint(process.memory_info().rss/1024**3)  # in bytes\n\n0.23577880859375\n\n\n\n# ramgb?"
  },
  {
    "objectID": "ipynb_snippets.html#vanilla",
    "href": "ipynb_snippets.html#vanilla",
    "title": "ipynb_snippets",
    "section": "Vanilla (?)",
    "text": "Vanilla (?)\n\n# Still nothing like parfor...\n\n\ni = -1\nimport sys,copy\nfrom functools import partial\ndef fun(arg_dt, something='ergerg'):\n    sys._getframe(1).f_locals.update(**arg_dt)\n    #locals().update(**arg_dt) # &lt;-- will not work\n    q=i+1\n    return q\n# inspect.currentframe().f_locals['foo'] = 'bar'\nlst = []  ; lst2 = []; lst3 = []\nfor i in range(5):\n    ndt = dict()\n    ndt.update(**locals())\n    ndt['i'] = copy.copy(ndt['i'])*1.0\n    lst2.append(ndt)\n    lst3.append(fun)\n    lst.append(partial(lst3[-1], lst2[-1])) # Partial stuff does not work with locals trickery\nprint(list(map(lambda x: x(), lst)))\nprint(list(map(fun,lst2)))\n\n[5, 5, 5, 5, 5]\n[1.0, 2.0, 3.0, 4.0, 5.0]\n\n\n\nfrom threading import Thread\ntdt = {}\narg = 3\ndef fun():\n    time.sleep(1)\n    res = arg + 5\n    tdt['res'] = res\n    return res\n\ntobj = Thread(target=fun)\ntt = tobj.start()\n\n\nr = tobj.join()\n\n\ntobj.\n\n&lt;Thread(Thread-6 (fun), stopped 139627026589248)&gt;\n\n\n\n# tt\n\n\nfun()\n\n8\n\n\n\n# ergerg\n# from multiprocessing import Pool \nfrom multiprocessing.pool import ThreadPool, Pool\nimport time\n\ntry:\n    pool.close()\nexcept:\n    True\n# pool = Pool(10)\npool = ThreadPool(10)\nprint('this')\n\nnanidx_dt = {}\n\ndef foo_pool(i, iid, slice_arr):\n    nanidx_sampi_arr = np.nonzero(int8_trn_sda.val[i,:].copy() == -127)[0]\n#     nanidx_sampi_arr = np.nonzero(slice_arr == -127)[0]\n    return i, '_'.join(iid), nanidx_sampi_arr\n\ndef log_result(res):\n    i , iid_key, slice_arr = res\n    # This is called whenever foo_pool(i) returns a result.\n    # result_list is modified only by the main process, not the pool workers.\n#     print('inside :',i)\n    nanidx_dt[iid_key] = slice_arr\n#     nanidx_lst.append(slice_arr)\n\nfor i, iid in enumerate(int8_trn_sda.iid):\n    rr = pool.apply_async(foo_pool, args = (i, iid, 0), callback=log_result)\n    if np.mod(i, 2000) == 0:\n        print('this: ', i, end='\\r')\n        time.sleep(0.02)\n        \nprint(f't-step:{len(tc_lst)}', time.time()-tc_lst[-1]); tc_lst.append(time.time());\nwhile len(nanidx_dt.keys()) &lt; (200000-5):\n    time.sleep(5)\n    print('bing', end=', ')\n    \nprint(f't-step:{len(tc_lst)}', time.time()-tc_lst[-1]); tc_lst.append(time.time());\npool.close()\npool.join()\n\nthis\n\n\nNameError: name 'int8_trn_sda' is not defined\n\n\n\nfrom concurrent.futures import ThreadPoolExecutor #, ProcessPoolExecutor\npool = ThreadPoolExecutor(4)\n\ntdt = {}\ndef fun(arg):\n    time.sleep(1)\n    res = arg*1.9 + 5\n    tdt[arg] = res\n    return res + 3\n\nfut = pool.submit(fun, arg)\nprint(fut.result(), '(Takes a second)')\n\n9.9 (Takes a second)\n\n\n\nfut_lst = []\narg_lst = [1,2,3,4,5,6,7,8,9]\n\n\ntdt = {}\ndef fun(arg):\n    time.sleep(10)\n    res = arg*1.9 + 5\n    tdt[arg] = res\n    return res + 3\n\nfor arg in arg_lst:\n    fut = pool.submit(fun, arg)\n    fut_lst.append(fut)\n\n\nprint(pool._work_queue.qsize())\nres_lst = [fut.result() for fut in fut_lst]\nprint(res_lst, '(Takes seconds)')\n\n5\n[9.9, 11.8, 13.7, 15.6, 17.5, 19.4, 21.299999999999997, 23.2, 25.099999999999998] (Takes seconds)\n\n\n\n# the option for which there is no code here:\nconcurrent.futures"
  },
  {
    "objectID": "ipynb_snippets.html#submitit-pipeline",
    "href": "ipynb_snippets.html#submitit-pipeline",
    "title": "ipynb_snippets",
    "section": "Submitit pipeline",
    "text": "Submitit pipeline\n\nBasic setup\n\n# Function to execute on cluster:\ndef run_blrjob(model=None):\n    tic(); print(model)\n    model.fit(X,y)\n    toc()\n    return model\n\n\n# Executor:\nlog_folder = \"/home/mennow/dsmwpred/mennow/log_test/%j\"\nexecutor = submitit.AutoExecutor(folder=log_folder)\nexecutor.update_parameters(slurm_mem='100G', cpus_per_task=15, slurm_time='10:04:00',slurm_additional_parameters={'account': 'risk_prediction'}) \n#slurm_account='risk_prediction')\n\n# Jobs specs:\njob_dt = {}\nfor key, model in model_dt.items():\n    job_dt[key] = executor.submit(run_blrjob, model=model)\n\n# Check:\nprint('and after:') \n!que\n\n\n!que # Should I add this to my mjwt package?\n\n/bin/bash: line 1: que: command not found\n\n\n\n\nParameter Grid\n\n# Approach version I:\n\nfrom sklearn.model_selection import ParameterGrid\n# [chaindt(minidt for minidt in dt.values()) for dt in params_lst]\ndef nans2nones(in_df):\n    isna = in_df.isna().values\n    vals = in_df.values; vals[isna] = None\n    return pd.DataFrame(vals, index=in_df.index, columns=in_df.columns)\n\n\nld_lst = [\n    dict(cm=0.01, shift=0), #0\n    dict(cm=2.0,  shift=0), #3\n    dict(cm=10.0, shift=0), #6\n]\n\nfold_lst = [\n    dict(fold='val', geno_fn='UKBB.val'),\n    dict(fold='test', geno_fn='UKBB.test')\n]\n\ndtype_lst = [\n    dict(dtype='float32'),\n    dict(dtype='float64')\n]\n\ndt = dict(\n    ld    = ld_lst,\n    fold  = fold_lst,\n    dtype = dtype_lst\n)\n\nparams_lst = list(ParameterGrid(dt))\ndf = pd.DataFrame(params_lst)\nparam_df = pd.concat([df[cols].apply(lambda x: pd.Series(x, dtype='object')) for cols in df.columns], axis=1)\nparam_df = nans2nones(param_df)\n# param_df.T.to_dict()\nparam_df.head()\n\n\n\n\n\n\n\n\ndtype\nfold\ngeno_fn\ncm\nshift\n\n\n\n\n0\nfloat32\nval\nUKBB.val\n0.01\n0\n\n\n1\nfloat32\nval\nUKBB.val\n2.0\n0\n\n\n2\nfloat32\nval\nUKBB.val\n10.0\n0\n\n\n3\nfloat32\ntest\nUKBB.test\n0.01\n0\n\n\n4\nfloat32\ntest\nUKBB.test\n2.0\n0\n\n\n\n\n\n\n\n\n# Approach version II:\n\n# Grid Hyperparams:\nn_reps = 1\nn_iter_arr = np.array([10])\nh2_arr     = np.round(np.logspace(log(0.05),log(0.9),8),3)\nrandom_state = 42\nif not 'L2Pred' in locals(): # Trick\n    class L2Pred: pass\n\n# Create param_df:\nparam_df = pd.DataFrame([0],columns=['null'])\n\n# Assign fun:\ndef assign_fun(arg_df, **kwargs):\n    arg_df = arg_df.copy()\n    for key,item in kwargs.items():\n        arg_df[key] = [item] * arg_df.shape[0]\n    return arg_df\n\n# Data specs:\ndataspec_lst = []\nfor i in range(n_reps):\n    dataspec_lst = dataspec_lst + [\n        (dict(random_state=random_state+i, pheno=pheno)) for pheno in ['PRCA','TURBO']\n    ]\nparam_df = pd.concat([assign_fun(param_df, dwargs=dkwargs)\n                     for dkwargs in dataspec_lst]\n                    ).reset_index(drop=True)\n# Model specs:\nmodelspec_lst = [\n    *((L2Pred, dict(something='this', h2=h2, n_iter=n_iter)) for h2 in h2_arr for n_iter in n_iter_arr),\n    *((L2Pred, dict(something='that', h2=h2, n_iter=n_iter)) for h2 in h2_arr for n_iter in n_iter_arr)\n]\nparam_df = pd.concat([assign_fun(param_df, mclass=mclass, mkwargs=mkwargs)\n                     for mclass, mkwargs in modelspec_lst]\n                    ).reset_index(drop=True)\n\ndisplay(param_df.head())\n\n\n\n\n\n\n\n\nnull\ndwargs\nmclass\nmkwargs\n\n\n\n\n0\n0\n{'random_state': 42, 'pheno': 'PRCA'}\n&lt;class '__main__.L2Pred'&gt;\n{'something': 'this', 'h2': 0.05, 'n_iter': 10}\n\n\n1\n0\n{'random_state': 42, 'pheno': 'TURBO'}\n&lt;class '__main__.L2Pred'&gt;\n{'something': 'this', 'h2': 0.05, 'n_iter': 10}\n\n\n2\n0\n{'random_state': 42, 'pheno': 'PRCA'}\n&lt;class '__main__.L2Pred'&gt;\n{'something': 'this', 'h2': 0.076, 'n_iter': 10}\n\n\n3\n0\n{'random_state': 42, 'pheno': 'TURBO'}\n&lt;class '__main__.L2Pred'&gt;\n{'something': 'this', 'h2': 0.076, 'n_iter': 10}\n\n\n4\n0\n{'random_state': 42, 'pheno': 'PRCA'}\n&lt;class '__main__.L2Pred'&gt;\n{'something': 'this', 'h2': 0.114, 'n_iter': 10}\n\n\n\n\n\n\n\n\n# Approach version III:\n# Grid Hyperparams:\nn_reps = 1\nn_iter_arr = np.array([10])\nh2_arr     = np.round(np.logspace(log(0.05),log(0.9),8),3)\nrandom_state = 42\nif not 'L2Pred' in locals(): # Trick\n    class L2Pred: pass\n    \n# Data specs:\ndataspec_lst = []\nfor i in range(n_reps):\n    dataspec_lst = dataspec_lst + [\n        (dict(random_state=random_state+i, pheno=pheno)) for pheno in ['PRCA','TURBO']\n    ]\n\n# Model specs:\nmodelspec_lst = [\n    *((L2Pred, dict(something='this', h2=h2, n_iter=n_iter)) for h2 in h2_arr for n_iter in n_iter_arr),\n    *((L2Pred, dict(something='that', h2=h2, n_iter=n_iter)) for h2 in h2_arr for n_iter in n_iter_arr)\n]\n\ndt = dict(\ndwargs = dataspec_lst,\nmkwargs = modelspec_lst,\n)\ngrid = pd.DataFrame(ParameterGrid(dt))\ngrid.head()\n\n\n\n\n\n\n\n\ndwargs\nmkwargs\n\n\n\n\n0\n{'random_state': 42, 'pheno': 'PRCA'}\n(&lt;class '__main__.L2Pred'&gt;, {'something': 'this', 'h2': 0.05, 'n_iter': 10})\n\n\n1\n{'random_state': 42, 'pheno': 'PRCA'}\n(&lt;class '__main__.L2Pred'&gt;, {'something': 'this', 'h2': 0.076, 'n_iter': 10})\n\n\n2\n{'random_state': 42, 'pheno': 'PRCA'}\n(&lt;class '__main__.L2Pred'&gt;, {'something': 'this', 'h2': 0.114, 'n_iter': 10})\n\n\n3\n{'random_state': 42, 'pheno': 'PRCA'}\n(&lt;class '__main__.L2Pred'&gt;, {'something': 'this', 'h2': 0.173, 'n_iter': 10})\n\n\n4\n{'random_state': 42, 'pheno': 'PRCA'}\n(&lt;class '__main__.L2Pred'&gt;, {'something': 'this', 'h2': 0.261, 'n_iter': 10})\n\n\n\n\n\n\n\n\n\nJob processing\n\n# # Define Internal Executor:\ndebug = False; print('debug =', debug)\nfolder = \"../lnk/menno/log_test/%j\"\nexecutor = submitit.AutoExecutor(folder=folder)\nexecutor.update_parameters(slurm_mem='119G', cpus_per_task=15, slurm_time='11:54:00',\n                           slurm_additional_parameters={'account': 'NCRR'})\n\nif debug:\n    make_brd_df = lambda : brd \nelse:\n    def make_brd_df(fn='../results/betas/final/all-betas.pst.h5'):\n        brd = PstHdf5(fn)\n        return brd\ndef debug_make_pheno_df(fold='test'):\n    return pheno_df\nwrapped_make_pheno_df = debug_make_pheno_df if debug else make_pheno_df\n\njob_dt = dict()\nfor i, cfg_dt in param_df.T.to_dict().items(): \n\n    def ppb_fun():\n        xp_dt = experimental_setup(geno_fn=cfg_dt['geno_fn'], fold=cfg_dt['fold'], shift=cfg_dt['shift'], gb_size_limit=19., cm=cfg_dt['cm'], dtype=cfg_dt['dtype']) \n        pheno_df = wrapped_make_pheno_df(fold=cfg_dt['fold'])\n        brd = make_brd_df()\n\n        tst_prd = Pheno(dict(iid=pheno_df.index.to_frame()[['fid','iid']].values.astype(str),\n                   vals=pheno_df.values, header=list(pheno_df.columns)))\n        cur_srd, cur_prd = pstutil.intersect_apply([xp_dt['tst_srd'], tst_prd])\n\n        pgm = MultiPGSModel(brd=brd, verbose=True, dtype=cfg_dt['dtype'])\n        pred_dt = pgm.predict(srd=cur_srd, prd=cur_prd)\n\n        linkdata = xp_dt['tst_linkdata']\n        mc = PrivacyPreservingMetricsComputer(linkdata=linkdata, brd=brd, s=pred_dt['s'], Bm=pred_dt['Bm'], cov_method='local', dtype=cfg_dt['dtype'], clear_linkage=False)\n        mcres_dt = mc.evaluate(debug=debug)\n        \n        C = corr(pred_dt['Ytru'].astype(cfg_dt['dtype']), pred_dt['Yhat'].astype(cfg_dt['dtype']))\n        \n        res_dt = dict(brd=pred_dt['brd'], C=C,\n                      Bm=pred_dt['Bm'], stansda=pred_dt['stansda'], \n                      mcres_dt=mcres_dt, linkdata=linkdata)\n        \n        return res_dt\n                           \n    job_dt[i] = executor.submit(ppb_fun)\n    print('submitted: ', i, end='\\r')\nprint('done')\neval_job_dt = job_dt"
  },
  {
    "objectID": "ipynb_snippets.html#profling",
    "href": "ipynb_snippets.html#profling",
    "title": "ipynb_snippets",
    "section": "Profling",
    "text": "Profling\n\nimport line_profiler\n\n\n# other example:\n# %lprun -f experimental_setup experimental_setup(decomp_fun=vanilla_svd, filter_str='minitest')\n# Also if there is no code showm, perhaps try to update package line_profiler\n\nModuleNotFoundError: No module named 'line_profiler'\n\n\n\n# or\n\n\ndef fun():\n    n= 100\n    X = np.random.randn(n,n)\n    D = X.T@X/n\n    for i in range(50):\n        a,b,c = linalg.svd(D)\n        invt_chol = linalg.cholesky(D + np.eye(n)*0.01)\n        \nprof = %lprun -r -f fun res=fun()\nprof.print_stats()\n\nTimer unit: 1e-09 s\n\nTotal time: 0.14446 s\nFile: /tmp/ipykernel_1752/2008979480.py\nFunction: fun at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def fun():\n     2         1       1112.0   1112.0      0.0      n= 100\n     3         1     357956.0 357956.0      0.2      X = np.random.randn(n,n)\n     4         1     463782.0 463782.0      0.3      D = X.T@X/n\n     5        50      45637.0    912.7      0.0      for i in range(50):\n     6        50  125506374.0 2510127.5     86.9          a,b,c = linalg.svd(D)\n     7        50   18085045.0 361700.9     12.5          invt_chol = linalg.cholesky(D + np.eye(n)*0.01)\n\n\n\n\nfor j in tqdm(range(6)):\n    time.sleep(1)"
  },
  {
    "objectID": "ipynb_snippets.html#timing-experiments",
    "href": "ipynb_snippets.html#timing-experiments",
    "title": "ipynb_snippets",
    "section": "Timing Experiments:",
    "text": "Timing Experiments:\n\npystr = \"\"\"\nimport os\nos.environ['OPENBLAS_NUM_THREADS'] = '{n_cpu}'\n# os.environ\ndef return_fun(this_np):\n    n_trix = {n_trix}\n    X = this_np.random.randn(n_trix,n_trix)\n    def fun():\n        for i in range(80):\n            s = X.dot(X)\n    return fun\n\nimport numpy as np\nfun = return_fun(np)\nfun()\n\"\"\"\n# import os\n# # os.environ[\"OMP_NUM_THREADS\"] = \"4\" # export OMP_NUM_THREADS=4\n# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=4 \n# # os.environ[\"MKL_NUM_THREADS\"] = \"4\" # export MKL_NUM_THREADS=6\n# # os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\" # export VECLIB_MAXIMUM_THREADS=4\n# # os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\" # export NUMEXPR_NUM_THREADS=6\n\n\ndef fun(n_cpu, n_trix):\n    import subprocess\n    proc = subprocess.run(\"python -c \\\"\" + pystr.format(n_cpu=n_cpu, n_trix=n_trix) + \"\\\"\", shell=True, check=False, capture_output=True)\n#     print(proc.stdout.decode('utf-8'))\n#     print(proc.stderr.decode('utf-8'))\n    \nn_trix = 1000\nfor i in [10,5,2,1]:\n    print(i)\n\n10\n2.74 s ± 131 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n5\n2.77 s ± 132 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n2\n2.74 s ± 132 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n1\n4.87 s ± 49.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "ipynb_snippets.html#slurm-submitit-tricks",
    "href": "ipynb_snippets.html#slurm-submitit-tricks",
    "title": "ipynb_snippets",
    "section": "Slurm & Submitit Tricks",
    "text": "Slurm & Submitit Tricks\n‘job_dt’ contains dictionary with SlurmJob() objects from the package submitit. How to make these I might specify later.\n\nfor i, job in job_dt.items():\n    job.cancel()\n\n\n# Cancel ALL submitit jobs!:\nfrom io import StringIO\nstring =  !que\nstringio = StringIO('\\n'.join(string))\nque_df = pd.read_table(stringio, delim_whitespace=True); que_df\n\nfor _, jobid in que_df.JOBID[que_df.NAME == 'submitit'].iteritems():\n    stop # raise error to prevent effects of sloppy copy pasting.\n    !scancel {jobid}\n\nUse printouts from job_dt to reinstate the job_dt and continue analysis:\n\nstring = \"\"\"60080867, 60080868, 60080869, 60080870, 60080871, 60080872, \"\"\"\nlog_folder = \"/home/mennow/dsmwpred/mennow/log_test/%j\"\neval_job_dt = dict()\nfor i, job_id in enumerate(string.split(', ')):\n    if job_id != '': eval_job_dt[i] = submitit.SlurmJob(job_id=job_id, folder=log_folder)\n\n\nstring='''{0: SlurmJob&lt;job_id=55037710, task_id=0, state=\"COMPLETED\"&gt;,\n 1: SlurmJob&lt;job_id=55037711, task_id=0, state=\"COMPLETED\"&gt;,\n 2: SlurmJob&lt;job_id=55037712, task_id=0, state=\"RUNNING\"&gt;,\n 3: SlurmJob&lt;job_id=55037713, task_id=0, state=\"RUNNING\"&gt;,\n 4: SlurmJob&lt;job_id=55037714, task_id=0, state=\"COMPLETED\"&gt;,\n 5: SlurmJob&lt;job_id=55037715, task_id=0, state=\"COMPLETED\"&gt;,\n 6: SlurmJob&lt;job_id=55037716, task_id=0, state=\"RUNNING\"&gt;,\n 7: SlurmJob&lt;job_id=55037717, task_id=0, state=\"RUNNING\"&gt;,\n 8: SlurmJob&lt;job_id=55037718, task_id=0, state=\"COMPLETED\"&gt;,\n 9: SlurmJob&lt;job_id=55037719, task_id=0, state=\"COMPLETED\"&gt;,\n 10: SlurmJob&lt;job_id=55037720, task_id=0, state=\"COMPLETED\"&gt;,\n 11: SlurmJob&lt;job_id=55037721, task_id=0, state=\"RUNNING\"&gt;,\n 12: SlurmJob&lt;job_id=55037722, task_id=0, state=\"RUNNING\"&gt;,\n 13: SlurmJob&lt;job_id=55037723, task_id=0, state=\"RUNNING\"&gt;,\n 14: SlurmJob&lt;job_id=55037724, task_id=0, state=\"RUNNING\"&gt;,\n 15: SlurmJob&lt;job_id=55037725, task_id=0, state=\"RUNNING\"&gt;}'''\n\nfolder = \"/home/mennow/dsmwpred/mennow/log_test/%j\"\njob_dt = dict()\nfor i, elem in enumerate(string.split('\\n')):\n    start = 'job_id='\n    stop = ', task_id='\n    job_id = elem[elem.find(start)+len(start):elem.find(stop)]\n    job_dt[i] = submitit.SlurmJob(job_id=job_id, folder=folder)\n\n\n# Clean out everything older then 9 days modded. Careful with this!\n!find ./log_test/* -type d -ctime +9 -exec rm -rf {} \\;\n\n\nstring = !squeue -t pd\n# string = !squeue -t r # For RUNNING ones.\nstring = '\\n'.join(string)\nfrom io import StringIO\ndf = pd.read_csv(StringIO(string), delim_whitespace=True)\ndf.groupby('USER').count().sort_values(by='JOBID').iloc[::-1]\n\n# If you really want all: this cell is a little buggy...\nstring = !squeue -u mennow\nstring = '\\n'.join(string)--format=\"%all\"\nfrom io import StringIO\ndf = pd.read_csv(StringIO(string), delim_whitespace=True)\n\n# Seeing some prints:\n[print(f'---&gt; i={i} &lt;---\\n', job_dt[i].stdout()[-100:]) for i in np.sort(np.random.randint(0,len(job_dt), 4)) if job_dt[i].stdout() is not None]\n\n# See a unique count of all the Slurm states:\ndf = pd.DataFrame([elem.state for elem in eval_job_dt.values()])\ndf[1] = 1.\ndf.groupby(0).count()\n\n\n# Resubmission of Jobs:\nresub_lst = []\nfor key, job in eval_job_dt.items():x\n    if job.state == 'FAILED':\n        new_job = executor.submit(job.submission().function)\n        eval_job_dt[key] = new_job\n        resub_lst.append(key)\n        print(key, job)\nprint('DONE')\n\n\n# Unpack job subs:\n# Get those configs back:\njob.submission().function.__globals__['cfg_dt']\n\n# It seems that the source can actually get lost\ninspect.getsource(job.submission().function) # DOES NOT WORK after some time."
  },
  {
    "objectID": "ipynb_snippets.html#javascript-html-formatting",
    "href": "ipynb_snippets.html#javascript-html-formatting",
    "title": "ipynb_snippets",
    "section": "Javascript, HTML, formatting",
    "text": "Javascript, HTML, formatting\n\n# useful for getting dataframes on the same row, will mess up the rest of the nb (not cell specific)\n# display(HTML(f\"&lt;style&gt;.cell-{durr} .output {{flex-direction: row;}}&lt;/style&gt;\"))\n# display(HTML('&lt;style&gt;.output {flex-direction: row;}&lt;/style&gt;'))\ndisplay(HTML('&lt;style&gt;.output {flex-direction: column;}&lt;/style&gt;'))\n\n\n\n\n\n# Get the cell ID\ndef get_cell_id():\n    shell = get_ipython()\n    if shell:\n        return shell.execution_count\n    return None\n\n# Example usage\ncell_id = get_cell_id()\nprint(\"Cell ID:\", cell_id)\n# print(_i78)\n\n\nprint(_i,_ii,_iii)\nshell = get_ipython()\nr=1\n\n\nvar output_area = this;\n// find my cell element\nvar cell_element = output_area.element.parents('.cell');\n// which cell is it?\nvar cell_idx = Jupyter.notebook.get_cell_elements().index(cell_element);\n// get the cell object\nvar cell = Jupyter.notebook.get_cell(cell_idx);\n// IPython.notebook.kernel.execute(`myvar = '${encodeURI(cell)}'`);\nIPython.notebook.kernel.execute(`myvar = '${cell_idx}'`);\nconsole.log(\"jkhergkjhkjge mennow message\")\n\n\nmyvar='empty'\nstring = \"\"\"\nvar output_area = this;\n// find my cell element\nvar cell_element = output_area.element.parents('.cell');\n// which cell is it?\nvar cell_idx = Jupyter.notebook.get_cell_elements().index(cell_element);\n// get the cell object\nvar cell = Jupyter.notebook.get_cell(cell_idx);\n// IPython.notebook.kernel.execute(`myvar = '${encodeURI(cell)}'`);\nIPython.notebook.kernel.execute(`myvar = '${cell_idx}'`);\n\"\"\"\ndisplay(Javascript(string))\nprint(myvar) # Problem with this thing is that it is one behind. \n# Need to wait for the cell to execute. Prob. some GIL issues thing.\n\n\n\n\nempty\n\n\n\n# now it does contain the cell location:\nprint(myvar)\n\n179\n\n\nUse HTML commands for images inorder to be able to resize them!\n&lt;div&gt;&lt;img src=\"attachment:image.png\" alt=\"bbla ballablal\" style=\"max-width: 20%\"&gt;&lt;/div&gt;\n\nThis also works:\n&lt;div style=\"width:25%;\"&gt;\n    \n![image.png](attachment:image.png)\n&lt;/div&gt;\nUse HTML commands for images inorder to be able to resize them!\n\n\n\nThis also works:\n\n\n\n\nimage.png\n\n\n\n&lt;p class='lead'&gt;\nThis is a nice looking little bigger text.\n&lt;/p&gt;\n\nThis is a nice looking little bigger text.\n\n\n#not sure what you can do with this\nHTML('''\n&lt;style&gt;\ndiv.warn {    \n    background-color: #fcf2f2;\n    border-color: #dFb5b4;\n    border-left: 5px solid #dfb5b4;\n    padding: 0.5em;\n    }\n&lt;/style&gt;\n''')"
  },
  {
    "objectID": "ipynb_snippets.html#create-your-own-magic",
    "href": "ipynb_snippets.html#create-your-own-magic",
    "title": "ipynb_snippets",
    "section": "Create your own magic!",
    "text": "Create your own magic!\n\nfrom IPython.core.magic import register_cell_magic\n\n\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)"
  },
  {
    "objectID": "ipynb_snippets.html#tricks",
    "href": "ipynb_snippets.html#tricks",
    "title": "ipynb_snippets",
    "section": "Tricks",
    "text": "Tricks\n\n# The thing to do:\n\n\n# Run this line if you want to get some help infos:\n??implot\n\n\n# blr.alpha_1\n\n\n# Reimporting things (not sure it works like desired..) # debuggin needed\nfrom importlib import reload  \n# import mjwt\n# mjwt=reload(mjwt)\nimport mjwt.utils\nmjwt.utils=reload(mjwt.utils)\nfrom mjwt.utils import printfun\n\n\n# self\n\n\n\n\nself=BayesianRidge()\ndt['some_fun'].__name__=funkynamed_fun\n{'__abstractmethods__': frozenset(), '__annotations__': {'_parameter_constraints': &lt;class 'dict'&gt;}, '__class__': &lt;class 'sklearn.linear_model._bayes.B  ... etc.\n\n\n1\n\n\n\n# _dh\n\n\n# Get previous outputs or inputs:\nprint(_); # or [__,___]\nprint(In[-1]) # Or inputs\n\n1\n# Get previous outputs or inputs:\nprint(_); # or [__,___]\nprint(In[-1]) # Or inputs\n\n\n\ndef fun(cut):\n    for i in range(5):\n        if i&gt;cut:\n            print('I break-ed'); break\n    else: print('I did not break') #convergence\nfun(2)\nfun(8)\n\nI break-ed\nI did not break\n\n\n\n# comment - durr now\nif notebook:\n    print(In[-1])\n\n# comment - durr now\nif notebook:\n    print(In[-1])\n\n\n\n# Get the cell ID\ndef get_cell_id():\n    shell = get_ipython()\n    if shell:\n        return shell.execution_count\n    return None\n\n# Example usage\ncell_id = get_cell_id()\nprint(\"Cell ID:\", cell_id)\n# print(_i78)\n\nCell ID: 136"
  },
  {
    "objectID": "ipynb_snippets.html#under-the-hoodys",
    "href": "ipynb_snippets.html#under-the-hoodys",
    "title": "ipynb_snippets",
    "section": "Under the hoodys:",
    "text": "Under the hoodys:\n\n# Leet hierarchy surfing:\ndef funkynamed_fun():\n    return 'text'\ndt = {'some_fun':funkynamed_fun}\nobj = BayesianRidge()\nmethod = blr.fit\n# vars() like stuff\nself = method.__self__\n# print(f\"{self:=}\")\nprint(f\"{self=:}\")\n\n# get the name of a function as variable\nprint(f\"{dt['some_fun'].__name__=:}\")\n\nobj_dt = {field:getattr(obj,field) for field in dir(obj)}\nprint(repr(obj_dt)[:150], ' ... etc.');1 # print obj_dt\n\nself=BayesianRidge()\ndt['some_fun'].__name__=funkynamed_fun\n{'__abstractmethods__': frozenset(), '__annotations__': {'_parameter_constraints': &lt;class 'dict'&gt;}, '__class__': &lt;class 'sklearn.linear_model._bayes.B  ... etc.\n\n\n1\n\n\n\n# Magic!! vars()\nprint(implot.__code__.co_varnames)\nprint(inspect.getfullargspec(implot))\n\n('cmap', 'midpoint', 'title', 'figsize', 'f', 'ncols', 'nrows', 'vmin', 'vmax', 'show', 'interpolation', 'args', 'kwargs', 'frame', 'code', 'arr', 'admin', 'commas', 'num', 'titles_dt', 'aspect', 'suffix_title', 'height', 'cnt', 'fig', 'axes', 'nn', 'ax', 'vals', 'mid', 'delta', 'im')\nFullArgSpec(args=[], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=['cmap', 'midpoint', 'title', 'figsize', 'f', 'ncols', 'nrows', 'vmin', 'vmax', 'show', 'interpolation'], kwonlydefaults={'cmap': 'seismic', 'midpoint': &lt;function median&gt;, 'title': None, 'figsize': None, 'f': 1.4, 'ncols': 1, 'nrows': 1, 'vmin': None, 'vmax': None, 'show': True, 'interpolation': 'none'}, annotations={})\n\n\n\nimport sklearn\nfrom sklearn import linear_model\ndisplay(inspect.getmro(linear_model.BayesianRidge.__class__))\nprint(inspect.getmro(linear_model.BayesianRidge.__class__)) # get class inheritance stuffies\n# assert LinkageData in linkdata.__class__.__mro__ # moar mro things\n\n(abc.ABCMeta, type, object)\n\n\n(&lt;class 'abc.ABCMeta'&gt;, &lt;class 'type'&gt;, &lt;class 'object'&gt;)\n\n\n\nprint(dir(__builtins__))\n\n['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BaseExceptionGroup', 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning', 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError', 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EncodingWarning', 'EnvironmentError', 'Exception', 'ExceptionGroup', 'False', 'FileExistsError', 'FileNotFoundError', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError', 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'ModuleNotFoundError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError', 'RecursionError', 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning', 'StopAsyncIteration', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '__IPYTHON__', '__build_class__', '__debug__', '__doc__', '__import__', '__loader__', '__name__', '__package__', '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__', '__spec__', 'abs', 'aiter', 'all', 'anext', 'any', 'ascii', 'bin', 'bool', 'breakpoint', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'display', 'divmod', 'enumerate', 'eval', 'exec', 'execfile', 'filter', 'float', 'format', 'frozenset', 'get_ipython', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'range', 'repr', 'reversed', 'round', 'runfile', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars', 'zip']\n\n\n\nfrom pathlib import Path\nss = Path(inspect.getsourcefile(sizegb)).read_text()"
  },
  {
    "objectID": "ipynb_snippets.html#argparse",
    "href": "ipynb_snippets.html#argparse",
    "title": "ipynb_snippets",
    "section": "argparse",
    "text": "argparse\n\npython -c \"\"\"\n\nimport sys\nfrom argparse import ArgumentParser\n\nparser = ArgumentParser()\n#parser.add_argument('-c', '--call', type=str)\nparser.add_argument('-b','--ball', type=int, default=42, help='this is ball')\nparser.add_argument('-a','--arg', type=str, default='defastr', help='this is the a') #, action='store')\n#parser.add_argument('-a', name='valstr')\n#parser.add_argument('-a','--arg', name='valstr', action='stoor')\n\nparser.add_argument('-c', type=str)\nparser.add_argument('--xpname', type=str, required=True)\n\nprint('parser:\\n', parser)\nargs = parser.parse_args()\n\nprint('args: \\n',args)\nprint(args.xpname)\n\nprint(sys.argv)\n\nprint('-- did all the red stuff in the cell above --')    \n\"\"\" --xpname blablkabkl\n\nparser:\n ArgumentParser(prog='-c', usage=None, description=None, formatter_class=&lt;class 'argparse.HelpFormatter'&gt;, conflict_handler='error', add_help=True)\nargs: \n Namespace(ball=42, arg='defastr', c=None, xpname='blablkabkl')\nblablkabkl\n['-c', '--xpname', 'blablkabkl']\n-- did all the red stuff in the cell above --"
  },
  {
    "objectID": "ipynb_snippets.html#bash",
    "href": "ipynb_snippets.html#bash",
    "title": "ipynb_snippets",
    "section": "Bash",
    "text": "Bash\n\n# How to send commands to commandline and get sensical strings back:\nimport subprocess\ninfo = subprocess.check_output('ls -atlr', shell=True).decode(\"utf-8\") # decode to go from bytestring to string\nprint(str(info))\n# not sure why this would be preferable over: !do things {insert_string_from_python}; do more things\n\ntotal 5224\n-rw-r--r--  1 jovyan users   55320 Oct  9  2020 coef_df.h5\n-rw-r--r--  1 jovyan users     290 Jun  9 12:50 _quarto.yml\n-rw-rw-r--  1 jovyan users    2217 Jun 14 13:51 index.ipynb\n-rw-rw-r--  1 jovyan users   17248 Jun 15 12:41 00_utils.ipynb\ndrwxr-xr-x  8 jovyan users     256 Jun 15 15:00 _archiv\ndrwxr-xr-x  6 jovyan users     192 Jun 15 15:01 .ipynb_checkpoints\n-rw-rw-r--  1 jovyan users     620 Jun 15 16:11 styles.css\n-rw-r--r--  1 jovyan users     276 Jun 15 18:02 nbdev.yml\n-rw-r--r--  1 jovyan users     106 Jun 15 18:02 sidebar.yml\ndrwxr-xr-x 20 jovyan users     640 Jun 15 18:03 ..\n-rw-r--r--  1 jovyan users    6148 Jun 15 18:17 .DS_Store\n-rw-r--r--  1 jovyan users 5008780 Jun 16 18:02 ipynb_snippets.ipynb\ndrwxr-xr-x 13 jovyan users     416 Jun 16 18:02 ."
  },
  {
    "objectID": "ipynb_snippets.html#decoration",
    "href": "ipynb_snippets.html#decoration",
    "title": "ipynb_snippets",
    "section": "Decoration",
    "text": "Decoration\n\n# Methods that overwrite the getting and setting syntax\nclass C(object):\n    def __init__(self):\n        self._x = None\n        self.data = 42\n\n    @property\n    def x(self):\n        \"\"\"I'm the 'x' property.\"\"\"\n        print(\"getter of x called\")\n        return self._x\n\n    @x.setter\n    def x(self, value):\n        print(\"setter of x called\")\n        self._x = value\n\n    @x.deleter\n    def x(self):\n        print(\"deleter of x called\")\n        del self._x\n        \n\nc = C()\nc.x = 'foo'  # setter called\nfoo = c.x    # getter called\ndel c.x      # deleter called\n\nsetter of x called\ngetter of x called\ndeleter of x called"
  },
  {
    "objectID": "ipynb_snippets.html#package-creation-stuff",
    "href": "ipynb_snippets.html#package-creation-stuff",
    "title": "ipynb_snippets",
    "section": "Package creation stuff",
    "text": "Package creation stuff\n\nA template: https://github.com/RobertoPrevato/PythonTemplate\n\n\n# Make a requirements.txt automatically\n!pipreqs ~/proj/mjwt/ # --force\n\nWARNING: Import named \"scipy\" not found locally. Trying to resolve it at the PyPI server.\nWARNING: Import named \"scipy\" was resolved to \"scipy:1.10.1\" package (https://pypi.org/project/scipy/).\nPlease, verify manually the final list of requirements.txt to avoid possible dependency confusions.\nINFO: Successfully saved requirements file in /home/jovyan/proj/mjwt/requirements.txt\n\n\n\n# !pip install pipreqs"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "# display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n\n\n# %config Completer.use_jedi = False\n_isdevenv_prstools = True\n\n\n# rger\n\n\nsource\n\nResultsStorage\n\n ResultsStorage (timer, suffix='xp', res_base_dn='./results/',\n                 verbose=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\npcorr\n\n pcorr (X, Y, ignore_nans=False)\n\n\nsource\n\n\ncorr\n\n corr (X, Y=None, ddof=0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\n\n\n\n\n\nY\nNoneType\nNone\n\n\n\nddof\nint\n0\nso a correlation might not actually have a ddof.\n\n\n\n\nsource\n\n\njobinfo\n\n jobinfo (job, return_string=False)\n\n\nsource\n\n\npsrc\n\n psrc (obj, return_source=False)\n\nPrint the code of a Python object\n\nsource\n\n\nStruct\n\n Struct (dt=None)\n\n\nsource\n\n\ndo_all_above\n\n do_all_above ()\n\n\nsource\n\n\nredo_all_above\n\n redo_all_above ()\n\n\nsource\n\n\nimplot\n\n implot (*args, cmap='seismic', midpoint=&lt;function median at\n         0x7f3c5f3c60f0&gt;, title=None, figsize=None, f=1.4, ncols=4,\n         nrows=1, vmin=None, vmax=None, show=True, interpolation='none',\n         **kwargs)\n\n\nsource\n\n\nlegimplot\n\n legimplot (M, show=True, cmap=None, interpolation='none', title=None,\n            figsize=None, **kwargs)\n\n\nsource\n\n\nfind_varname\n\n find_varname (var)\n\n\nsource\n\n\nTimer\n\n Timer ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nbeep\n\n beep (seconds=1.0, framerate=4410, v=0.8, rate=1, pitch=1.0,\n       fn='completion_2.mp3')\n\n\nsource\n\n\nsizegb\n\n sizegb (var, verbose=False)\n\n\nsource\n\n\nfullvars\n\n fullvars (obj)\n\n\nsource\n\n\nTree\n\n Tree ()\n\n\nsource\n\n\nsuppress_warnings\n\n suppress_warnings (func)\n\n\nsource\n\n\nprintfun\n\n printfun (arg='ergergreg')\n\n\n1\n\n\n# plt.colormaps.register\n\n\n1\n\n\n1\n\n\n1\n\n\nprint('something')\n\n\n!nbdev_export\n# !nbdev_prepare\n# !sleep 0.1\n# !head -32 ../mjwt/utils.py\n\n\n!\n\n\n# ??nbdev.nbdev_export\n\nThe next cell does a bunch of tests:\n\ntest_eq('magicstringg','magicstringg')\ntest_eq(41+1,42)\n\nEnd of test cells."
  },
  {
    "objectID": "cmd_snippets.html",
    "href": "cmd_snippets.html",
    "title": "cmd_snippets",
    "section": "",
    "text": "!git add cmd_snippets.ipynb; git commit -m \"savety update\""
  },
  {
    "objectID": "cmd_snippets.html#general",
    "href": "cmd_snippets.html#general",
    "title": "cmd_snippets",
    "section": "General",
    "text": "General\n.bash_profile is executed once at startup. .bashrc is executed with every new terminal.\nreadlink -f linkname\ngroups mennow\ngetent group | grep mennow\nfinger vang # Soeren Vang &lt;vang@ki.au.dk\nscp ~/DDownloads/environment.yml mennow@login.genome.au.dk:/home/mennow/cmd/\ncat /proc/cpuinfo | grep -i flags\n\n\n# Piping (it is a stream!)\nprogram &&gt;&gt; result.txt\n#equals\nprogram &gt;&gt; result.txt 2&gt;&1\n#\n| stdin AND &gt; std out\n\nwatch -n 0.1 tail output_stream.txt\n\n# cd to latest dir:\ncd $(ls -td -- */ | head -n 1)\n\n\n# this\n\nwhile true; do clear; tail -100  $(ls -td -- */ | head -n 1)output_stream.txt ; sleep 1; done\ncat /proc\n# this\n\nwatch -n 0.3 tail -30 `bash -c \"ls -td -- */output_stream.txt | head -n 1\"`\nwatch -n 0.3 tail -n 7 `bash -c \"ls -td -- */output_stream.txt | head -n 70 | grep lasso\"`\n\n\n\njupyter nbconvert --to script run_pipeline.ipynb; python run_pipeline.py --xpname devcmd --code run_lasso_xp.ipynb\n\n# For folder size and to see the total:\ndu -csh *\n# And this is you want it sorted:\ndu -csh * | sort -h"
  },
  {
    "objectID": "cmd_snippets.html#tar-gzip",
    "href": "cmd_snippets.html#tar-gzip",
    "title": "cmd_snippets",
    "section": "Tar & gzip:",
    "text": "Tar & gzip:\n\n# removes .gz &lt;-- MJW's preference\ngunzip PGS000013.txt.gz\n\n# Also works\ngzip -d PGS000013.txt.gz\n\n\n# Create a tar file:\ntar -czvf output.tar.gz ~/folder/\n\n# Untar a badboi:\ntar -xzvf input.tar.gz"
  },
  {
    "objectID": "cmd_snippets.html#cleaning",
    "href": "cmd_snippets.html#cleaning",
    "title": "cmd_snippets",
    "section": "Cleaning",
    "text": "Cleaning\n# Clean out everything older then 9 days modded. Careful with this!\nfind ./log_test/* -type d -ctime +9 -exec rm -rf {} \\;"
  },
  {
    "objectID": "cmd_snippets.html#tmux",
    "href": "cmd_snippets.html#tmux",
    "title": "cmd_snippets",
    "section": "tmux",
    "text": "tmux\n# ctrl+b d to detach\n\nC-b p previous window\nC-b n next window\nC-b ( previous session\nC-b ) next session"
  },
  {
    "objectID": "cmd_snippets.html#vim",
    "href": "cmd_snippets.html#vim",
    "title": "cmd_snippets",
    "section": "Vim",
    "text": "Vim\n\nEscape, control + f (page down) control + b (page up)"
  },
  {
    "objectID": "cmd_snippets.html#finding-stuff-find-grep",
    "href": "cmd_snippets.html#finding-stuff-find-grep",
    "title": "cmd_snippets",
    "section": "Finding Stuff (find & grep):",
    "text": "Finding Stuff (find & grep):\n# Find filename with \"imp\" string present and remove permission denied:\nfind . -name \"*imp*\" 2&gt;&1 | grep -v 'Permission denied' &gt; ~/find_result.txt\n\n# Very cool: exclude geno from wc and then grep multi-or-pattern the result:\nwc -l $(ls | grep -v geno.bed) | grep \"train\\|test\"\n\n#https://stackoverflow.com/questions/16956810/how-do-i-find-all-files-containing-specific-text-on-linux\ngrep --include=\\*.{c,ipynb} -rnw '/path/to/somewhere/' -e 'pattern'\n\n# Search all lower dirs for lines with \".__\" (-F == not regular expression)\ngrep -rn -F \".__\"\n\n# do an OR quickly:\ngrep -e PATTERN1 -e PATTERN2"
  },
  {
    "objectID": "cmd_snippets.html#magic-tricks",
    "href": "cmd_snippets.html#magic-tricks",
    "title": "cmd_snippets",
    "section": "Magic tricks",
    "text": "Magic tricks\n# random grabs of stuff\nshuf -n 10\n\n# pipe arguments to commands\nxargs\n\n# Column selection and string manipulation:\nawk '{print \"./prscs/\"$4}'"
  },
  {
    "objectID": "cmd_snippets.html#ssh-and-auth",
    "href": "cmd_snippets.html#ssh-and-auth",
    "title": "cmd_snippets",
    "section": "SSH and Auth:",
    "text": "SSH and Auth:\n# Generate Ultra secure RSA keypair 4 github:\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" -f ~/.ssh/id_rsa_github\n\n# For github neatness in ~/.ssh/config\nHost github.com\n HostName github.com\n IdentityFile ~/.ssh/id_rsa_github\n# additional chmod prob. required: https://superuser.com/questions/232373/how-to-tell-git-which-private-key-to-use"
  },
  {
    "objectID": "cmd_snippets.html#conda",
    "href": "cmd_snippets.html#conda",
    "title": "cmd_snippets",
    "section": "Conda:",
    "text": "Conda:\n# Create conda yml environment file:\nconda env export --name dck &gt; dck.yml\n\n# Install Udo's magical conda things:\nconda install -c udogi dark-matter\n\n# This is for making a real clone:\nconda create --name dckclone --clone dck"
  },
  {
    "objectID": "cmd_snippets.html#jupyter",
    "href": "cmd_snippets.html#jupyter",
    "title": "cmd_snippets",
    "section": "Jupyter:",
    "text": "Jupyter:\n\n# Add conda env to jupyter (it worx :)\n# Dont forget to create env from base\nconda activate dckclone # &lt;-- crucial!!\nconda install -c anaconda ipykernel\n\n# This is how you can add the kernel to the jupyter space:\npython -m ipykernel install --user --name=dckclone\n\n# Do this to find out if it is all working:\n#import sys\n#print(sys.executable)\n# This is how you can list your jupyter kernels:\njupyter kernelspec list\n\n# This is how you can remove them:\njupyter kernelspec uninstall myenv"
  },
  {
    "objectID": "cmd_snippets.html#basics",
    "href": "cmd_snippets.html#basics",
    "title": "cmd_snippets",
    "section": "Basics",
    "text": "Basics\n# This is for pulling in an image from docker hub.\ndocker pull jupyter/datascience-notebook\n\n# An example of to how run an image:\ndocker run -it --rm -v $(realpath ~/notebooks):/tf/notebooks -p 127.0.0.1:8888:8888 tensorflow/tensorflow:latest-jupyter\n# a crucial additon being the \"127.0.0.1:\" this makes sure it is only linked to localhost.\n\n# Another example for running an image:\ndocker run -it -p 8888:8888 tensorflow/tensorflow \n\n# DOCKER RUN options:\n-i, --interactive                    Keep STDIN open even if not attached\n-c, --cpu-shares int                 CPU shares (relative weight)\n-d, --detach                         Run container in background and print container ID\n## More advanced docker stuff:\n\n# Extract Image ID from container(s): This will give you an incredibly verbose json containing info on image/container. I think this is what is called the 'manifest'.\ndocker inspect $(container or image id)\n\n# This gives you a full hash of which the first part is going to be equal to the image ID\ndocker inspect --format='{{.Id}} {{.Name}} {{.Image}}' $(docker ps -aq)\n\n#BREAK INTO THE CONTAINER WITH ROOT :\ndocker exec -itu root docker-compbio-ds bash\n% docker exec -it -u root 700d4360d263c0222b73f44138d4a1dce8b6df46d6ae7775530db3297dcec53d bash\n\n# Use this for running docker-ds:\ndocker exec -it docker-ds /bin/bash\n\n–&gt; Example Dockerfile from MJW:\n\n# Stuff after : is a digest, does this work?\n#FROM jupyter/datascience-notebook@sha256:b672f926e0f2ddb4b68172d33c0fea64f8f66651e86e233dc12894ddf7299b98\nFROM jupyter/datascience-notebook:dc9744740e12\n# Works too but you have 2 go onto dockerhub. It can be excessively painfull to find the 'latest' back on docker hub.\n\nCOPY requirements.txt /tmp/\n\nRUN pip install -r /tmp/requirements.txt && \\\n    fix-permissions $CONDA_DIR && \\\n    fix-permissions /home/$NB_USER\n    \nRUN jupyter contrib nbextension install --user"
  },
  {
    "objectID": "cmd_snippets.html#building-updating",
    "href": "cmd_snippets.html#building-updating",
    "title": "cmd_snippets",
    "section": "Building & Updating",
    "text": "Building & Updating\n\n# Way to build from your new pretty Dockerfile:\ncd ~/proj/docker/jupyter-base-ds\ndocker build --rm -t mennowitteveen/my-datascience-notebook-1 .\n\n# After running this, rebooting the docker session will result in an updated session.\n#jupyter snippet that will save new environment and stuff it into hte git repo.\n\n!pip freeze &gt; /home/jovyan/proj/docker/jupyter-base-ds/requirements.txt\n\n!git config --global user.email \"\" \n!git config --global user.name  \"Menno Witteveen\"\n!git -C /home/jovyan/proj/docker/jupyter-base-ds/ add requirements.txt\n# !git -C /home/jovyan/proj/docker/jupyter-base-ds/ status\n!git -C /home/jovyan/proj/docker/jupyter-base-ds/ commit -m \"update req.txt with line_profiler\""
  },
  {
    "objectID": "cmd_snippets.html#options-parameters",
    "href": "cmd_snippets.html#options-parameters",
    "title": "cmd_snippets",
    "section": "Options & Parameters",
    "text": "Options & Parameters\n\n# Running docker with different RAM & SWAP\n--memory and --memory-swap"
  },
  {
    "objectID": "cmd_snippets.html#general-1",
    "href": "cmd_snippets.html#general-1",
    "title": "cmd_snippets",
    "section": "General:",
    "text": "General:\n\n\nFor Pull Requests (PR) : [WIP] work in progress [MRG] ready to merge\nPR’s will automatically update. so you donnot have to close and open the one, if it is not accepted right away.\nFor sklearn every new feature has to have its own test.\npytest seems to be an important element of testing, flake8 check syntax i think and is used in sklearn\n\n\n# To update your form master branch from its base: (Now I dont understand this anymore)\ngit pull upstream master\n\n# Just see what changed:\ngit diff\n# Start\nconda install git\ngit init .\n\n# Unstaging stuff:\ngit reset -- notebooks/data/human\n\n# New branch creation, from specific commit:\ngit checkout -b new_branch branchhashhjgejhg464565jhergkjherg\n\n# git list all branches:\ngit branch -a"
  },
  {
    "objectID": "cmd_snippets.html#working-with-multiple-branches",
    "href": "cmd_snippets.html#working-with-multiple-branches",
    "title": "cmd_snippets",
    "section": "Working with multiple branches:",
    "text": "Working with multiple branches:\nTips: https://stackoverflow.com/questions/20101994/how-to-git-pull-from-master-into-the-development-branch\ngit checkout dev      # gets you \"on branch dev\"\ngit fetch origin        # gets you up to date with origin\ngit merge origin/main   # main or master, depending."
  },
  {
    "objectID": "cmd_snippets.html#properly-merge-uncommited-changes-on-different-locations",
    "href": "cmd_snippets.html#properly-merge-uncommited-changes-on-different-locations",
    "title": "cmd_snippets",
    "section": "Properly Merge uncommited changes on different locations:",
    "text": "Properly Merge uncommited changes on different locations:\n\n# Step 1: Commit changes on location of choice and push to remote\n\n# Step 2: \"Stash\" changes on other location\ngit stash\n\n# Step 3: Pull commited changes from the other location\ngit pull\n\n# Step 4: Magically have uncommitted changes to files:\ngit stash pop\n\n# Step 5: Commit & push\ngit add file.py\ngit commit -m \"example\"\ngit push\n\nSource: https://stackoverflow.com/questions/19216411/how-do-i-pull-files-from-remote-without-overwriting-local-files\nhttps://stackoverflow.com/questions/1125968/how-do-i-force-git-pull-to-overwrite-local-files"
  },
  {
    "objectID": "cmd_snippets.html#listing-information-history-tracked-files",
    "href": "cmd_snippets.html#listing-information-history-tracked-files",
    "title": "cmd_snippets",
    "section": "Listing Information (History, tracked files):",
    "text": "Listing Information (History, tracked files):\n# To list previous commits:\ngit log\n\n# See a list of all currently tracked files:\ngit ls-tree -r master --name-only\ngit config —-list\n\ngit config —-global user.email menno@mennowitteveen.com\n\ngit help config OR git config —help\n\ngit init —&gt; makes .git dir\nrm -rf .git —&gt; removes the complete thing!\n\ngit status"
  },
  {
    "objectID": "cmd_snippets.html#create-github-repo-and-push",
    "href": "cmd_snippets.html#create-github-repo-and-push",
    "title": "cmd_snippets",
    "section": "Create Github repo and push:",
    "text": "Create Github repo and push:\npasta\ntouch README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:mennowitteveen/&lt;reponame&gt;.git\ngit push -u origin master"
  },
  {
    "objectID": "cmd_snippets.html#personal-acces-token-pat-stuff",
    "href": "cmd_snippets.html#personal-acces-token-pat-stuff",
    "title": "cmd_snippets",
    "section": "Personal Acces token (PAT) stuff:",
    "text": "Personal Acces token (PAT) stuff:\n\nPersonal access tokens are finniky!! You need to log out and update a bunch of time before these things start to work…\ngit clone *a thing*\nusername: &lt;username&gt;\npassword: *gph_AKEYTHINGKJRJKERSUHSU*"
  },
  {
    "objectID": "cmd_snippets.html#github-integration-stuff",
    "href": "cmd_snippets.html#github-integration-stuff",
    "title": "cmd_snippets",
    "section": "Github integration stuff:",
    "text": "Github integration stuff:\nYou need to make sure your username and email match. with that it works and your profile pic shines next to your commits.\ngit config --global user.name \"mennowitteveen\"\ngit config --global user.email menno.j.eveen@emailulike.com"
  },
  {
    "objectID": "cmd_snippets.html#extras",
    "href": "cmd_snippets.html#extras",
    "title": "cmd_snippets",
    "section": "Extras:",
    "text": "Extras:\n\n#Colors:\ngit config --global color.ui auto\n!git -C /home/mennow/proj/lambdapred rev-parse HEAD \nThere is a way to make git push faster.\nTry creating a directory ~/.ssh/control and then adding this to your ~/.ssh/config:\nHost github.com\n  ControlMaster auto\n  ControlPath ~/.ssh/control/%r@%h:%p\n  ControlPersist 3600\nhttps://stackoverflow.com/questions/14170722/how-to-make-git-push-run-faster\nGit pull until there is something to pull:\nhttps://stackoverflow.com/questions/24404238/git-pull-until-there-is-something-to-pull"
  },
  {
    "objectID": "cmd_snippets.html#general-2",
    "href": "cmd_snippets.html#general-2",
    "title": "cmd_snippets",
    "section": "General",
    "text": "General\n\n# Download stuff cmd on mac:\ncurl -O https://storage.googleapis.com/finngen-public-data-r6/summary_stats/R6_manifest.tsv\n\n# link hardlink ln, for hardlinking a dir do:\n# OOPS SO THAT STUFF DOES NOT INSTALL SINCE CERTAIN APPLE OS, NOW DID: ln ./source/* ./destdir/\nhln source dest"
  },
  {
    "objectID": "cmd_snippets.html#shortcuts",
    "href": "cmd_snippets.html#shortcuts",
    "title": "cmd_snippets",
    "section": "Shortcuts",
    "text": "Shortcuts\n\nshortcuts –&gt; control command M for math\n5 x option to enable mousekeys (usefull for drag selection)\ncommand + control + Q for locking!"
  },
  {
    "objectID": "cmd_snippets.html#iterm",
    "href": "cmd_snippets.html#iterm",
    "title": "cmd_snippets",
    "section": "iTerm",
    "text": "iTerm\n\ncontrol a & e to jump around\noption arrow key (needs to be configure in the beginnign)\nProton VPN CLI: https://medium.com/@jiurdqe/protonvpn-command-line-tool-for-mac-os-564235c90caf"
  },
  {
    "objectID": "cmd_snippets.html#brew",
    "href": "cmd_snippets.html#brew",
    "title": "cmd_snippets",
    "section": "Brew",
    "text": "Brew\n\nIf brew has a mess up with the credentials:\n\nDelete the entry corresponding to git-credential-osxkeychain in Keychain Access and re-enter your credentials to get it working.\n\n\n\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:mennowitteveen/&lt;reponame&gt;.git\ngit push -u origin master\n\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:mennowitteveen/&lt;reponame&gt;.git\ngit push -u origin master\n\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:mennowitteveen/&lt;reponame&gt;.git\ngit push -u origin master\n\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:mennowitteveen/&lt;reponame&gt;.git\ngit push -u origin master\n\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:mennowitteveen/&lt;reponame&gt;.git\ngit push -u origin master"
  },
  {
    "objectID": "cmd_snippets.html#slurm",
    "href": "cmd_snippets.html#slurm",
    "title": "cmd_snippets",
    "section": "Slurm",
    "text": "Slurm\nPrint info on the Slurm account:\n# gater infration on user\nsacctmgr -s show user $USER format=user,account,maxjobs,maxsubmit,qos\n\n# Get infos on last jobs:\nsacct -u imennow --starttime=2022-04-04 -t\n\n# Cool stuff that is leet, remove all submitit jobs:\nque | grep submitit | awk '{print $1}' | xargs scancel"
  },
  {
    "objectID": "cmd_snippets.html#genomedk",
    "href": "cmd_snippets.html#genomedk",
    "title": "cmd_snippets",
    "section": "GenomeDK",
    "text": "GenomeDK\n\nGeneral\ngnodes\njobinfo &lt;jobid&gt;\nscancel &lt;jobid&gt;\nscontrol show node s10n01 # infos on --contraint that can be set\n# Regarding piping: Sometimes things go wrong with the port piping. I discovered that refreshing .ssh/known_hosts fixes issues. \n\n\nInstalling\nAn install I did on some day:\n# This adds miniconda3 in home folder and adds conda stuff to .bashrc!\ncd ~/downloads\nwget https://repo.continuum.io/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh -O miniconda.sh\nchmod +x miniconda.sh\nbash miniconda.sh -b\n ~/miniconda3/bin/conda init bash\n\n# Mind: These channel Settings will remain persistent somewhere after deletion of miniconda3 folder:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --add channels genomedk\n\n# !conda env export --no-builds --name base &gt; environment.yml\n# scp ~/DDownloads/environment.yml mennow@login.genome.au.dk:/home/mennow/cmd/\n# scp ~/Downloads/notebook.json mennow@login.genome.au.dk:/home/mennow/cmd/\n\nconda env create --name dck -f ~/cmd/environment.yml\nconda activate dck\njupyter contrib nbextension install --user\ncp ~/cmd/notebook.json ~/.jupyter/nbconfig/\n \n\n\nExport (e.g. iPSYCH)\nMake a folder with the appropriate goodies.\nThen run:\ntar -czvf export-dd-mm-yyyy.tar.gz ./export-dd-mm-yyyy/\n# Important to double check\n## --&gt; you can use autocomplete to help out though.\ntar -xzvf export-dd-mm-yyyy.tar.gz\n# for the checks\nNext step is to run:\ngdk-export export-dd-mm-yyyy.tar.gz\nAfter this, an email should be send for which I have instructions in my mailbox.\n# for file server\nsftp menno@185.45.23.195"
  }
]